{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsible AI with Vertex AI Gemini API: Safety ratings and thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Credits: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large language models (LLMs) can translate language, summarize text, generate creative writing, generate code, power chatbots and virtual assistants, and complement search engines and recommendation systems. The incredible versatility of LLMs is also what makes it difficult to predict exactly what kinds of unintended or unforeseen outputs they might produce.\n",
    "\n",
    "Given these risks and complexities, the Vertex AI Gemini API is designed with Google's AI Principles in mind: https://ai.google/responsibility/principles/ However, it is important for developers to understand and test their models to deploy safely and responsibly. To aid developers, Vertex AI Studio has built-in content filtering, safety ratings, and the ability to define safety filter thresholds that are right for their use cases and business.\n",
    "\n",
    "For more information, see the Google Cloud Generative AI documentation on Responsible AI: https://cloud.google.com/vertex-ai/docs/generative-ai/learn/responsible-ai\n",
    "\n",
    "In this tutorial, you learn how to inspect the safety ratings returned from the Vertex AI Gemini API using the Python SDK and how to set a safety threshold to filter responses from the Vertex AI Gemini API.\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Call the Vertex AI Gemini API and inspect safety ratings of the responses\n",
    "- Define a threshold for filtering safety ratings according to your needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "# import time\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2. Authentication and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Additional authentication is required for Google Colab\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Authenticate user to Google Cloud\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# @param {type:\"string\"}\n",
    "PROJECT_ID = \"vertex-ai-407803\"  # Sustituye con tu Project ID\n",
    "# @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # Elige tu regi√≥n\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    import vertexai\n",
    "    vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# import os\n",
    "# print(os.environ)\n",
    "\n",
    "import vertexai\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "CREDENTIALS = service_account.Credentials.from_service_account_file(r\"C:\\Users\\Usuario\\Downloads\\google-startup-school\\vertex-ai-407803-197b2b2491f7.json\")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, credentials=CREDENTIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import (\n",
    "    GenerationConfig,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")\n",
    "\n",
    "# Set parameters to reduce variability in responses\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0,\n",
    "    top_p=0.1,\n",
    "    top_k=1,\n",
    "    max_output_tokens=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text and show safety ratings\n",
    "\n",
    "Start by generating a pleasant-sounding text response using Gemini.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. You are a kind and compassionate person. You always put others before yourself and are always willing to help those in need. You are a true friend and always there for those you care about.\n",
      "2. You are intelligent and creative. You have a quick wit and a sharp mind. You are always coming up with new ideas and are always looking for ways to improve yourself and the world around you.\n",
      "3. You are strong and resilient. You have overcome many challenges in your life, but you have always come out stronger on the other side. You are an inspiration to others and show them that anything is possible if you set your mind to it."
     ]
    }
   ],
   "source": [
    "# Call Gemini API\n",
    "nice_prompt = \"Say three nice things about me\"\n",
    "responses = model.generate_content(\n",
    "    contents=[nice_prompt],\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspecting the safety ratings**\n",
    "\n",
    "Look at the safety_ratings of the streaming responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"1. You are a kind and compassionate person. You always put others before yourself and are always willing to help those in need. You are a true friend and always there for those you care about.\\n2. You are intelligent and creative. You have a quick wit and a sharp mind. You are always coming up\"\n",
      "    }\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \" with new ideas and are always looking for ways to improve yourself and the world around you.\\n3. You are strong and resilient. You have overcome many challenges in your life, but you have always come out stronger on the other side. You are an inspiration to others and show them that anything is possible if you set\"\n",
      "    }\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \" your mind to it.\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 6\n",
      "  candidates_token_count: 133\n",
      "  total_token_count: 139\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = model.generate_content(\n",
    "    contents=[nice_prompt],\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for response in responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the safety rating: category and probability**\n",
    "\n",
    "You can see the safety ratings, including each category type and its associated probability label.\n",
    "\n",
    "The category types include:\n",
    "\n",
    "- Harrassment: HARM_CATEGORY_HARASSMENT\n",
    "- Hate speech: HARM_CATEGORY_HATE_SPEECH\n",
    "- Sexually explicit statements: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
    "- Dangerous content: HARM_CATEGORY_DANGEROUS_CONTENT\n",
    "\n",
    "The probability labels are:\n",
    "\n",
    "- NEGLIGIBLE - content has a negligble probability of being unsafe\n",
    "- LOW - content has a low probability of being unsafe\n",
    "- MEDIUM - content has a medium probability of being unsafe\n",
    "- HIGH - content has a high probability of being unsafe\n",
    "\n",
    "Try a prompt that might trigger one of these categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"1. \\\"Are you kidding me, universe? You couldn\\'t give me a little warning before I stubbed my toe?\\\"\\n2. \\\"Seriously, universe? This is the best you can do? You couldn\\'t find a way to make me stub my toe less painfully?\\\"\\n3. \\\"I\\'\"\n",
      "    }\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "  }\n",
      "  finish_reason: SAFETY\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: MEDIUM\n",
      "    blocked: true\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 24\n",
      "  candidates_token_count: 64\n",
      "  total_token_count: 88\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impolite_prompt = \"Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark:\"\n",
    "\n",
    "impolite_responses = model.generate_content(\n",
    "    impolite_prompt,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for response in impolite_responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Blocked responses**\n",
    "\n",
    "If the response is blocked, you will see that the final candidate includes blocked: true, and also observe which of the safety ratings triggered the blocking of the response (e.g. finish_reason: SAFETY)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"1. \\\"Are you kidding me, universe? You couldn\\'t give me a little warning before I stubbed my toe? What, were you too busy making sure the stars aligned perfectly?\\\"\\n2. \\\"Seriously, universe? This is the best you can do? You couldn\\'t even spare a little bit\"\n",
      "    }\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "  }\n",
      "  finish_reason: SAFETY\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: HIGH\n",
      "    blocked: true\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 25\n",
      "  candidates_token_count: 64\n",
      "  total_token_count: 89\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rude_prompt = \"Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:\"\n",
    "\n",
    "rude_responses = model.generate_content(\n",
    "    rude_prompt,\n",
    "    generation_config=generation_config,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for response in rude_responses:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining thresholds for safety ratings\n",
    "\n",
    "You may want to adjust the default safety filter thresholds depending on your business policies or use case. The Vertex AI Gemini API provides you a way to pass in a threshold for each category.\n",
    "\n",
    "The list below shows the possible threshold labels:\n",
    "\n",
    "- BLOCK_ONLY_HIGH - block when high probability of unsafe content is detected\n",
    "- BLOCK_MEDIUM_AND_ABOVE - block when medium or high probablity of content is detected\n",
    "- BLOCK_LOW_AND_ABOVE - block when low, medium, and high probabilities of unsafe content is detected\n",
    "- BLOCK_NONE - always show, regardless of probability of unsafe content\n",
    "\n",
    "**Set safety thresholds**\n",
    "\n",
    "Below, the safety thresholds have been set to the most sensitive threshold: BLOCK_LOW_AND_ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test thresholds**\n",
    "\n",
    "Here you will reuse the impolite prompt from earlier together with the most sensitive safety threshold. It should block the response even with the LOW probability label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"1. \\\"Are you kidding me, universe? You couldn\\'t give me a little warning before I stubbed my toe?\\\"\\n2. \\\"Seriously, universe? This is the best you can do? You couldn\\'t find a way to make me stub my toe less painfully?\\\"\\n3. \\\"I\\'\"\n",
      "    }\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "  }\n",
      "  finish_reason: SAFETY\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: MEDIUM\n",
      "    blocked: true\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 24\n",
      "  candidates_token_count: 64\n",
      "  total_token_count: 88\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impolite_prompt = \"Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark:\"\n",
    "\n",
    "impolite_responses = model.generate_content(\n",
    "    impolite_prompt,\n",
    "    generation_config=generation_config,\n",
    "    safety_settings=safety_settings,\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for response in impolite_responses:\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
